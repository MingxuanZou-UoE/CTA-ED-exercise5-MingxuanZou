---
title: "Exercise 5"
author: "Mingxuan Zou"
date: "`r Sys.Date()`"
output: html_document
---

#### 1. Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # loads dplyr, ggplot2, and others
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(topicmodels) # to estimate topic models
library(gutenbergr) # to get text data
library(scales)
library(tm)
library(ggthemes) # to make your plots look nice
library(readr)
library(quanteda)
library(quanteda.textmodels)
devtools::install_github("matthewjdenny/preText")
library(preText)
```

#### 2. Load data

```{r load data}
# Download the volumn 1 and 2
tocq  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/topicmodels/tocq.RDS?raw=true")))

# Reformat the data into document term matrix (dtm) in which col = term and row = doc.
tocq_words <- tocq %>%
  # create new cols to identify the term in questions as from volume 1 or volume 2
  mutate(booknumber = ifelse(gutenberg_id==815, "DiA1", "DiA2")) %>%
  # tokenization
  unnest_tokens(word, text) %>%
  # keep only words
  filter(!is.na(word)) %>%
  # sort by count of words in booknumber col
  count(booknumber, word, sort = TRUE) %>%
  ungroup() %>%
  # remove stop words
  anti_join(stop_words)

# Convert from tidy into dtm
tocq_dtm <- tocq_words %>%
  cast_dtm(booknumber, word, n)

tm::inspect(tocq_dtm)
```

#### 3. Estimate the LDA model
```{r}
# Estimate the LDA model
tocq_lda <- LDA(tocq_dtm, k = 10, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (β) that given term belongs to a given topic.
tocq_topics <- tidy(tocq_lda, matrix = "beta")

head(tocq_topics, n = 10)
```

#### 4. Plot the top terms
```{r}
# Identify top terms per topic
tocq_top_terms <- tocq_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) # sorts the results first by topic and then by beta in descending order (-beta)

# Visualise top terms per topic 
tocq_top_terms %>%
  # adjust the order of terms within each topic based on their beta values for better visualization
  mutate(term = reorder_within(term, beta, topic)) %>%
  # colour bars by topic
  ggplot(aes(beta, term, fill = factor(topic))) + 
  # plot the data as a column chart
  geom_col(show.legend = FALSE) + 
  # create a separate plot for each topic, allowing each to have its own y-axis scales. Arrange plots into 4 columns.
  facet_wrap(~ topic, scales = "free", ncol = 4) + 
  # ensure y-axis is ordered within each facet according to the reordered terms.
  scale_y_reordered() +
  theme_tufte(base_family = "TT Times New Roman")
```
#### 5. Evaluating topic model
##### 5.1. Plot relatively word frequencies
```{r}
# Examining characteristics that distinguish volume 1 and volume 2

# Convert the data frame into tidy format
tidy_tocq <- tocq %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

# Count most common words in both
tidy_tocq %>% 
  count(word, sort = TRUE)

# Calculate the proportions of words in two volumes relative to the whole book
bookfreq <- tidy_tocq %>% 
  # create new columns, booknumber = DiA1 if gutenberg_id==815, else booknumber = DiA2
  mutate(booknumber = ifelse(gutenberg_id==815, 'DiA1', 'DiA2')) %>% 
  # clean the word column by extracting only the lowercase alphabetical characters
  mutate(word = str_extract(word, '[a-z]+')) %>% 
  # group the data by columns and count the number of rows in each group
  count(booknumber, word) %>% 
  # group the dataset by the booknumber column
  group_by(booknumber) %>% 
  # create a new column that have the proportion of times each word appears relative to the total number of word occurrences in the book
  mutate(proportion = n / sum(n)) %>% 
  # remove the 'n' column from the dataset
  select(-n) %>% 
  # transform the dataset from long to wide format
  spread(booknumber, proportion)

# Plot
ggplot(bookfreq, aes(x = DiA1, y = DiA2, color = abs(DiA1 - DiA2))) + 
  # reference line 
  geom_abline(color = 'grey40', lty = 2) + 
  # plot the data points with a slight random variation in position to  make the distribution of points clearer
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + 
  # add text labels to points, avoid overlapping labels
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + 
  # transform x and y axes to a log scale
  scale_x_log10(labels = percent_format()) + 
  scale_y_log10(labels = percent_format()) + 
  # defines a gradient color scale for the points based on the difference between DiA1 and DiA2
  scale_color_gradient(limits = c(0, 0.001), low = 'darkslategray4', high = 'grey75') + 
  theme_tufte(base_family = "TT Times New Roman") + 
  theme(legend.position = 'none',
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
  labs(x = 'Tocqueville DiA 2', y = 'Tocqueville DiA 1') + 
  # ensures that x-ais and y-ais have the same unit length
  coord_equal()

# Overall, more abstract notions of political structure appear with greater frequency in Volume 1, (e.g. 'democratic', 'equality' and 'aristocracy') while Volume 2 seems to contain words specific to America, since 'americans' and 'america' has the greatest frequencies.


```

##### 5.2. Split into chapter documents
```{r}
# Separate the volumes into chapters

# Filter out missing values
tocq <- tocq %>% 
  filter(!is.na(text))

# Divide into docs, each representing one chapter
tocq_chapter <- tocq %>% 
  # create new columns, booknumber = DiA1 if gutenberg_id==815, else booknumber = DiA2
  mutate(booknumber = ifelse(gutenberg_id==815, 'DiA1', 'DiA2')) %>%
  group_by(booknumber) %>% 
  # numbers the chapters sequentially within each book
  mutate(chapter = cumsum(str_detect(text, regex('^chapter', ignore_case = TRUE)))) %>%
  ungroup() %>% 
  filter(chapter > 0) %>% 
  # combines the 'booknumber' and 'chapter' into a single 'document' column
  unite(document, booknumber, chapter)

# Split into words
tocq_chapter_word <- tocq_chapter %>% 
  unnest_tokens(word, text)

# Find doc-word counts
tocq_word_counts <- tocq_chapter_word %>% 
  anti_join(stop_words) %>% 
  count(document, word, sort = TRUE) %>% 
  ungroup()

tocq_word_counts

# Cast the results into DTM format for LDA analysis
tocq_chapters_dtm <- tocq_word_counts %>% 
  cast_dtm(document, word, n)

tm::inspect(tocq_chapters_dtm)

# Re-estimate the topic model with the new DTM, specifying k=2.
tocq_chapters_lda <- LDA(tocq_chapters_dtm, k = 2, control = list(seed = 1234))

# The per-doc-per-topic probability (γ), or the probability that a given doc (here:chapter) belongs to a particular topic (here:volume)
# The γ values are therefore the estimated proportion of words within a given chapter allocated to a given volume
tocq_chapters_gamma <- tidy(tocq_chapters_lda, matrix = 'gamma')
tocq_chapters_gamma
```

##### 5.3. Examine consensus
```{r}
# Examine topic model based on words contained in each chapter

# Separate the doc name into title and chapter
tocq_chapters_gamma <- tocq_chapters_gamma %>%
  separate(document, c('title', 'chapter'), sep = '_', convert = TRUE)

# Identify dominant topic by chapter
tocq_chapter_classifications <- tocq_chapters_gamma %>% 
  group_by(title, chapter) %>% 
  top_n(1, gamma) %>% 
  ungroup()

# Determine the dominant topic for each book
tocq_book_topics <- tocq_chapter_classifications %>% 
  count(title, topic) %>% 
  group_by(title) %>% 
  top_n(1,n) %>% 
  ungroup() %>% 
  transmute(consensus = title, topic)

# Filter chapters with topics diverging from book's consensus topic
tocq_chapter_classifications %>% 
  inner_join(tocq_book_topics, by = 'topic') %>% 
  filter(title != consensus)

# View doc-word pairs to examine the assignments
assignments <- augment(tocq_chapters_lda, data = tocq_chapters_dtm)
assignments

# Prepare and join data for visualisation
assignments <- assignments %>% 
  separate(document, c('title', 'chapter'), sep = '_', convert = TRUE) %>% 
  inner_join(tocq_book_topics, by = c('.topic' = 'topic'))

# visualise word-topic assignments
assignments %>% 
  count(title, consensus, wt = count) %>% 
  group_by(title) %>% 
  mutate(percent = n / sum(n)) %>% 
  ggplot(aes(consensus, title, fill = percent)) + 
  geom_tile() + 
  scale_fill_gradient2(high = 'red', label = percent_format()) + 
  geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) + 
  theme_tufte(base_family = 'TT Times New Roman') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        panel.grid = element_blank()) + 
  labs(x = 'Book words assigned to', 
       y = 'Book words came from', 
       fill = '% of assignments')
```
#### 6. Validation
```{r}
# Assess the impact of different pre-processing choices

# Reformat text into a quateda corpus object
# load in corpus of Tocequeville text data
corp <- corpus(tocq, text_field = 'text')
# use first 10 docs for example
documents <- corp[sample(1:30000, 1000)]
# view the doc names
print(names(documents[1:10]))

# Preprocess in alternative ways
preprocessed_documents <- factorial_preprocessing(
  documents, 
  use_ngrams = TRUE, 
  infrequent_term_threshold = 0.2, 
  verbose = FALSE)

# Compare the distance between docs that have been processed in different ways. (Approximately costs 8 mins)
preText_results <- preText(
  preprocessed_documents,
  dataset_name = 'Tocqueville text',
  distance_method = 'cosine', 
  num_comparisons = 20,
  verbose = FALSE)

# Plot the results
preText_score_plot(preText_results)
```

#### 7. Exercises
##### 7.1. Choose another book or set of books from Project Gutenberg
```{r choose James book, warning=FALSE, message=FALSE}
topsy <- gutenberg_download(c(57628,57634), meta_fields = "author")
#download two volumes of a book named "The Principles of Psychology" from Project Gutenberg


topsy_words <- topsy %>%
  mutate(booknumber = ifelse(gutenberg_id==57628, "VOL1", "VOL2")) %>% #create a new variable named 'booknumber'.If the id is 57628, then assigned it as 'VOL1'. If its id isn't 57628, then assigned it as 'VOL2'
  unnest_tokens(word, text) %>%            #token the text into single words
  filter(!is.na(word)) %>%                 #delete the rows without words
  count(booknumber, word, sort = TRUE) %>% #count the occurence of the combination of 'booknumber' and 'word', and the results are sorted in ascending order
  ungroup() %>%                            #remove the grouping structure 
  anti_join(stop_words)                    #remove stop words from the text


topsy_dtm <- topsy_words %>%               #create a new dataset 'topsy_dtm'
  cast_dtm(booknumber, word, n)            #cast data from the topsy_words into a document-term matrix format


tm::inspect(topsy_dtm)                    
#examine the frequency of terms across documents or any other relevant information stored in the matrix.
```

##### 7.2. Run your own topic model on these books, changing the k of topics, and evaluating accuracy.

In this part, I am running the LDA model on the dtm object created from the book "The Principles of Psychology", with topic number = 10 and = 20 in two iteration, respectively. The evaluation of these models are provided later in my part.

###### 7.2.1. Plot relative word frequencies
```{r k = 10}
# Estimate the LDA model

# 1. Number of topics = 10
topsy_lda_10 <- LDA(topsy_dtm, k = 10, control = list(seed = 1234))

# Extract the per-topic-per-word probabilities (β) that given term belongs to a given topic.
topsy_topics_10 <- tidy(topsy_lda_10, matrix = "beta")

# View the first 10 results
head(topsy_topics_10, n = 10)
```

```{r k = 20}
# 2. Number of topics = 20
topsy_lda_20 <- LDA(topsy_dtm, k = 20, control = list(seed = 1234))

topsy_topics_20 <- tidy(topsy_lda_20, matrix = "beta")

head(topsy_topics_20, n = 10)
```
The beta values represent the probability that the given term belongs to a given topic. By inspecting the beta values produced by two models, the term 'time' is most likely to belong to topic 2 for both model 1 and model 2. The top terms in terms of beta for each topic are plotted as follows.             
```{r plot top terms, warning=FALSE, message=FALSE}
# Identify top terms per topic for model 1
topsy_top_terms_10 <- topsy_topics_10 %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta) # sorts the results first by topic and then by beta in descending order (-beta)

# Visualise top terms per topic for model 1
topsy_top_terms_10 %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% # adjust the order of terms within each topic based on their beta values for better visualization
  ggplot(aes(beta, term, fill = factor(topic))) + # colour bars by topic
  geom_col(show.legend = FALSE) + # plot the data as a column chart
  facet_wrap(~ topic, scales = "free", ncol = 4) + # create a separate plot for each topic, allowing each to have its own y-axis scales. Arrange plots into 4 columns.
  scale_y_reordered() + # ensure y-axis is ordered within each facet according to the reordered terms.
  theme_tufte(base_family = "TT Times New Roman")
  
# Repeat for model 2
topsy_top_terms_20 <- topsy_topics_20 %>% 
  group_by(topic) %>% 
  top_n(10, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

topsy_top_terms_20 %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") + 
  scale_y_reordered() + 
  theme_tufte(base_family = "TT Times New Roman")
 
# Save the plot to a file with the desired size 
#ggsave("topsy_top20topics.png", plot = topsy_plot_20, width = 20, height = 20, units = "in", dpi = 400)
```

For evaluating the topic models for the book "The Principles of Psychology", I start from plotting relatively word frequencies to see whether there really are words obviously distinguishing the two Volumes.
```{r plot relative word frequencies, warning=FALSE, message=FALSE}
# Examining characteristics that distinguish volume 1 and volume 2

# Convert the data frame into tidy format
tidy_topsy <- topsy %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

# Count most common words in both
tidy_topsy %>% 
  count(word, sort = TRUE)

# Calculate the proportions of words in two volumes relative to the whole book
topsy_bookfreq <- tidy_topsy %>% 
  mutate(booknumber = ifelse(gutenberg_id==57628, 'TPoP1', 'TPoP2')) %>% # create new columns, booknumber = TPoP1 if gutenberg_id==57628, else booknumber = TPoP2
  mutate(word = str_extract(word, '[a-z]+')) %>% # clean the word column by extracting only the lowercase alphabetical characters
  count(booknumber, word) %>% # group the data by columns and count the number of rows in each group
  group_by(booknumber) %>% # group the dataset by the booknumber column
  mutate(proportion = n / sum(n)) %>% # create a new column that have the proportion of times each word appears relative to the total number of word occurrences in the book
  select(-n) %>% # remove the 'n' column from the dataset
  spread(booknumber, proportion) # transform the dataset from long to wide format

# Plot
ggplot(topsy_bookfreq, aes(x = TPoP1, y = TPoP2, color = abs(TPoP1 - TPoP2))) + 
  geom_abline(color = 'grey40', lty = 2) + # reference line
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + # plot the data points with a slight random variation in position to  make the distribution of points clearer
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + # add text labels to points, avoid overlapping labels
  scale_x_log10(labels = percent_format()) + # transform x and y axes to a log scale
  scale_y_log10(labels = percent_format()) + 
  scale_color_gradient(limits = c(0, 0.001), low = 'darkslategray4', high = 'grey75') + # defines a gradient colour scale for the points based on the difference between DiA1 and DiA2
  theme_tufte(base_family = "TT Times New Roman") + # specify theme elements and labels
  theme(legend.position = 'none',
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
  labs(x = 'James TPoP 2', y = 'James TPoP 1') + 
  coord_equal() # ensures that x-axis and y-axis have the same unit length
```

Terms such as 'eye', 'instinct', and 'hallucinations' appear with greater frequency in volume 1, indicating that this volume may have discussed the sensation and perception, which are significant topics in neuroscience or biological psychology. Meanwhile, Volume 2 is characterized by terms like 'consciousness', 'attention', and 'stream', and these are concepts central to psychoanalysis or cognitive psychology.

###### 7.2.2. Split into chapter documents

In the below, we first separate the volumes into chapters, then we repeat the same procedure as above. The only difference now is that instead of two documents representing the two full volumes of James’s work, we now have 84 documents, each representing an individual chapter. The sparsity has increased to (approximate) 95%.
```{r split into chap doc, warning=FALSE, message=FALSE}
# Separate the volumes into chapters

# Filter out missing values
topsy <- topsy %>% 
  filter(!is.na(text))

# Divide into docs, each representing one chapter
topsy_chapter <- topsy %>% 
  # create new columns, where booknumber = TPoP1 if gutenberg_id==57628, else booknumber = TPoP2
  mutate(booknumber = ifelse(gutenberg_id==57628, 'TPoP1', 'TPoP2')) %>%
  group_by(booknumber) %>% 
  # numbers the chapters sequentially within each book
  mutate(chapter = cumsum(str_detect(text, regex('^chapter', ignore_case = TRUE)))) %>%
  ungroup() %>% 
  filter(chapter > 0) %>% 
  # combines the 'booknumber' and 'chapter' into a single 'document' column
  unite(document, booknumber, chapter)

# Split into words
topsy_chapter_word <- topsy_chapter %>% 
  unnest_tokens(word, text)

# Find doc-word counts
topsy_word_counts <- topsy_chapter_word %>% 
  anti_join(stop_words) %>% 
  count(document, word, sort = TRUE) %>% 
  ungroup()

topsy_word_counts

# Cast the results into DTM format for LDA analysis
topsy_chapters_dtm <- topsy_word_counts %>% 
  cast_dtm(document, word, n)

tm::inspect(topsy_chapters_dtm)

# Re-estimate the topic model with the new DTM, specifying k=2.
topsy_chapters_lda <- LDA(topsy_chapters_dtm, k = 2, control = list(seed = 1234))

# The per-doc-per-topic probability (γ), or the probability that a given doc (here:chapter) belongs to a particular topic (here:volume)
# The γ values are therefore the estimated proportion of words within a given chapter allocated to a given volume
topsy_chapters_gamma <- tidy(topsy_chapters_lda, matrix = 'gamma')
topsy_chapters_gamma
```

###### 7.2.3. Examine consensus

Now that we have these topic probabilities, we can see how well our unsupervised learning did at distinguishing the two volumes generatively just from the words contained in each chapter.
```{r examine consensus, warning=FALSE, message=FALSE}
# Examine topic model based on words contained in each chapter

# Separate the doc name into title and chapter
topsy_chapters_gamma <- topsy_chapters_gamma %>%
  separate(document, c('title', 'chapter'), sep = '_', convert = TRUE)

# Identify dominant topic by chapter
topsy_chapter_classifications <- topsy_chapters_gamma %>% 
  group_by(title, chapter) %>% 
  top_n(1, gamma) %>% 
  ungroup()

# Determine the dominant topic for each book
topsy_book_topics <- topsy_chapter_classifications %>% 
  count(title, topic) %>% 
  group_by(title) %>% 
  top_n(1,n) %>% 
  ungroup() %>% 
  transmute(consensus = title, topic)

# Filter chapters with topics diverging from book's consensus topic
topsy_chapter_classifications %>% 
  inner_join(topsy_book_topics, by = 'topic') %>% 
  filter(title != consensus)

# View doc-word pairs to examine the assignments
assignments <- augment(topsy_chapters_lda, data = topsy_chapters_dtm)
assignments

# Prepare and join data for visualisation
assignments <- assignments %>% 
  separate(document, c('title', 'chapter'), sep = '_', convert = TRUE) %>% 
  inner_join(topsy_book_topics, by = c('.topic' = 'topic'))

# visualise word-topic assignments
assignments %>% 
  count(title, consensus, wt = count) %>% 
  group_by(title) %>% 
  mutate(percent = n / sum(n)) %>% 
  ggplot(aes(consensus, title, fill = percent)) + 
  geom_tile() + 
  scale_fill_gradient2(high = 'red', label = percent_format()) + 
  geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) + 
  theme_tufte(base_family = 'TT Times New Roman') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        panel.grid = element_blank()) + 
  labs(x = 'Book words assigned to', 
       y = 'Book words came from', 
       fill = '% of assignments')
```


##### 7.3. Validate different pre-processing techniques using preText on the new book(s) of your choice.
```{r validation, warning=FALSE, message=FALSE}
# Reformat text into a quateda corpus object
  # load in corpus of James text data
  corp <- corpus(topsy, text_field = 'text')
  # use first 10 docs for example
  documents <- corp[sample(1:30000, 1000)]
  # view the doc names
  print(names(documents[1:10]))

# Preprocess in alternative ways
preprocessed_documents_topsy <- factorial_preprocessing(
  documents, 
  use_ngrams = TRUE, # specify that n-grams should be preprocessed in addition to words and tokens
  infrequent_term_threshold = 0.2, # remove the terms that appear in fewer than 20% of the doc.
  verbose = FALSE) # 'verbose' mode prints out detailed progress or diagnostic messages. Silent operation by setting it to FALSE.
  
# Compare the distance between docs that have been processed in different ways. (Approximately 400s)
preText_results_topsy <- preText(
  preprocessed_documents_topsy,
  dataset_name = 'James text', # name the input dataset
  distance_method = 'cosine', # measure the cosine of angle between two vectors. Lower cos indicates higher similarity.
  num_comparisons = 20, # 20 pairs of preprocessed doc to be compared.
  verbose = FALSE) # likewise, for silent operation.

# Plot the results
preText_score_plot(preText_results_topsy)
```

